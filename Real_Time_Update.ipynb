{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\python311\\lib\\site-packages (0.2.40)\n",
      "Requirement already satisfied: schedule in c:\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: textblob in c:\\python311\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\python311\\lib\\site-packages (from yfinance) (1.25.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\python311\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\python311\\lib\\site-packages (from yfinance) (5.2.2)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\python311\\lib\\site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\python311\\lib\\site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\python311\\lib\\site-packages (from yfinance) (2.4.4)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\python311\\lib\\site-packages (from yfinance) (3.17.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\python311\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\python311\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\python311\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python311\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\python311\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\python311\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance schedule requests pandas textblob joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import joblib\n",
    "import schedule\n",
    "import time\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_keys = '5b654d218f124beeab121df62f4c4371'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['neg'], sentiment['neu'], sentiment['pos']\n",
    "\n",
    "def update_sentiment_analysis(df):\n",
    "    df[['Negative', 'Nuetral', 'Positive']] = df['content'].apply(\n",
    "        lambda x: pd.Series(analyze_sentiment(x)) if x else pd.Series([0, 0, 0])\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news(api_key, query, from_date, to_date, page_size=100):\n",
    "    url = f'https://newsapi.org/v2/everything?q={query}&from={from_date}&to={to_date}&pageSize={page_size}&apiKey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    articles = data.get('articles', [])\n",
    "    news_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Parse publishedAt to datetime format and convert to UTC\n",
    "        published_at = pd.to_datetime(article['publishedAt']).tz_convert('UTC')\n",
    "        \n",
    "        news_data.append({\n",
    "            'source': article['source']['name'],\n",
    "            'author': article['author'],\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': published_at,\n",
    "            'content': article['content']\n",
    "        })\n",
    "    \n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df = news_df.sort_values(by='publishedAt').reset_index(drop=True)  # Sort by publishedAt in ascending order\n",
    "    \n",
    "    # Update sentiment analysis\n",
    "    news_df = update_sentiment_analysis(news_df)\n",
    "\n",
    "\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_price(stock_symbol, from_stock_date, current_date):\n",
    "    stock = yf.Ticker(stock_symbol)\n",
    "    \n",
    "    # Convert dates to datetime format if they are not already\n",
    "    from_stock_date = pd.to_datetime(from_stock_date)\n",
    "    current_date = pd.to_datetime(current_date)\n",
    "    \n",
    "    # Get historical data for the specified date range with weekly frequency\n",
    "    historical_data = stock.history(start=from_stock_date, end=current_date, interval='1wk')\n",
    "    \n",
    "    # Extract 'Open' prices and reset index to get 'Date' as a column\n",
    "    historical_data = historical_data[['Open']].reset_index()\n",
    "    \n",
    "    # Rename columns to match the required format\n",
    "    historical_data.columns = ['Date', 'Stock Price']\n",
    "    \n",
    "    # Convert 'Date' to UTC to match 'publishedAt'\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Sort by 'Date' in ascending order\n",
    "    historical_data = historical_data.sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "    return historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Remove rows with missing values in critical columns\n",
    "    critical_columns = ['publishedAt', 'Date', 'content', 'Stock Price']\n",
    "    df = df.dropna(subset=critical_columns)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "    \n",
    "    # Ensure final sort order by publishedAt\n",
    "    df = df.sort_values(by='publishedAt', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save articles to CSV\n",
    "def save_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Examples\n",
    "\n",
    "\n",
    "In order for the program to work you need an API Key. Get your API Key from here: https://newsapi.org/\n",
    "\n",
    "You can change the query example a company name\n",
    "\n",
    "You can change the stock symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '' # you can register for an API Key from here: https://newsapi.org/\n",
    "query = 'Visa Inc' # You can change query e.g. company name\n",
    "from_date = datetime.now().strftime('%Y-%m-%d')\n",
    "to_date = '2020-06-26'\n",
    "from_stock_date = '2020-06-26'\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "stock_symbol =  'V' # You can change the stock symbol \n",
    "output_csv = f'Update{stock_symbol}__{current_date}_data.csv'\n",
    "\n",
    "# Fetch news data\n",
    "news_df = fetch_news(api_key, query, from_date, to_date)\n",
    "\n",
    "# Fetch stock price data\n",
    "stock_price_df = fetch_stock_price(stock_symbol, from_stock_date, current_date)\n",
    "\n",
    "# Merge stock price data into news_df\n",
    "merged_df = pd.merge_asof(news_df.sort_values('publishedAt'), stock_price_df,\n",
    "                          left_on='publishedAt', right_on='Date', direction='backward')\n",
    "\n",
    "# Ensure final sort order by publishedAt\n",
    "merged_df = merged_df.sort_values(by='publishedAt', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save merged data to CSV\n",
    "save_to_csv(merged_df, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
